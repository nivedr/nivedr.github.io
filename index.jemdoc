# jemdoc: menu{MENU}{index.html}
# jemdoc: analytics{UA-130203500-1}

= Nived Rajaraman
~~~
{}{img_left}{images/nived-SD.jpg}{Temp}{300}

[https://eecs.berkeley.edu/ Electrical Engineering and Computer Science Department],\n
[https://www.berkeley.edu/ University of California, Berkeley]\n
264 Cory Hall\n
Berkeley, CA 94720-1770\n
Email: nived \[dot\] rajaraman \[at\] berkeley \[dot\] edu
~~~

== About me

I am currently a second year Ph.D. student, affiliated with the BLISS and BAIR labs. My advisors are Profs. Jiantao Jiao and Kannan Ramachandran.

My research interests lie in the theory of reinforcement learning. I believe that a succesful work in this area proposes concrete theoretical formulations and ultimately provides intuitions missing in existing practical approaches.

I was previously a dual degree student at the [http://www.ee.iitm.ac.in/ Department of Electrical Engineering], [https://www.iitm.ac.in/ IIT Madras]. I was fortunate to have [https://www.microsoft.com/en-us/research/people/rakri/ Ravishankar Krishnaswamy] and [http://www.ee.iitm.ac.in/~andrew/ Prof. Andrew Thangaraj] as my master's thesis advisors.


== Select recent publications

1. [./papers/towards.html *Toward the Fundamental Limits of Imitation Learning*] (NeurIPS 2020) ([https://arxiv.org/abs/2009.05990 ArXiv]) ([https://crossminds.ai/video/toward-the-fundamental-limits-of-imitation-learning-606ff01ef43a7f2f827c1759/ video]) 

With /Lin F. Yang/, /Jiantao Jiao/ and /Kannan Ramchandran/

2. *Provably Breaking the Quadratic Error Compounding Barrier in Imitation Learning, Optimally* ([https://arxiv.org/abs/2102.12948 ArXiv])

With /Yanjun Han/, /Lin F. Yang/, /Kannan Ramchandran/ and /Jiantao Jiao/

With a known model, Mimic-MD provably breaks the error-compounding barrier in IL. However, it is a-priori unclear whether the dependence on the horizon, H^{3/2}, can be further improved given an accurate model. To this end, we establish a lower bound showing that for any learner which knows the MDP transition, there exists an IL instance on $|S| = 3$ states such that the suboptimality of the learner scales as $\Omega (H^{3/2}/N)$ with constant probability. This result shows that in terms of the horizon dependence, Mimic-MD is optimal in the worst case. The lower bound construction relies on a novel reduction to mean estimation with subsampled observations and establishing statistical limits for the same, which may be of independent interest.

Often in practice, the demonstrator turns out to carry out the task at hand very efficiently and is near optimal. In this work, we also explore IL under the additional restriction that the expert is an optimal policy under the true underlying reward function. This setting turns out to be quite challenging because of the highly non-convex reward structure imposed by the observations in the expert dataset. While it is trivial to construct good estimates of the expert state distribution, /realizing/ good state distributions via Markovian policies is a far more challenging problem. Achieving the latter is a necessary hurdle to overcome in coming up with policies with small suboptimality. In this paper, we propose an efficient algorithm, termed Mimic-Mixture, which can provably realize a near unbiased (upto an error of $O(1/N)$) state estimate of the state distribution of any single state in the MDP. As a consequence, for MDPs on 3-states with rewards only on the terminal layer, Mimic-Mixture returns a policy incurring suboptimality O(1/N).

In contrast, we show that no algorithm can achieve suboptimality $O(\sqrt{H/N})$ with high probability if the expert is not constrained to be optimal. Thus, our work formally establishes the benefit of imposing optimality of the expert when the model is known, which contrasts with the result in Rajaraman et al (2020) which shows that it does not help in the worst case, when the learner cannot interact with the environment.

3. *On the Value of Interaction and Function Approximation in Imitation Learning* (under submission)

With /Yanjun Han/, /Lin F. Yang/, /Jiantao Jiao/ and /Kannan Ramchandran/

In practice, often learners implement algorithms such as DAgger which actively query the expert while interacting with the environment. In contrast, our previous work studies approaches such as behavior cloning as well as Mimic-MD which passively interact with the expert. In the worst case Rajaraman et al (2020) establishes that there is no benefit of actively querying the expert if the model is not known. The lower bound relies on constructing an MDP with a "bad" state which is absorbing and offers no reward and showing that any learner visits this state with reasonable probability. This worst case example is pathological in the following sense: in practical situations such as driving a car, experts often can recover at states and collect a high reward even if a "mistake" is made locally. This assumption is captured by the $\mu$-recoverability assumption of Ross and Bagnell (2011) which assumes that the difference in the $Q$-value under the expert policy across different actions in a state does not deviate by more than $\mu$ from the maximum.

In addition, Ross and Bagnell (2011) propose a reduction showing that any policy which minimizes the $0$-$1$ loss with the expert action distribution to $\le \epsilon$ when the state is generated by the learner's own state distribution incurs suboptimality $\le \mu H \epsilon$ under this assumption. In this work, we show that this reduction is in fact statistically optimal - the resulting policy upon interacting with the MDP for $N$ episodes incurs a suboptimality of $O ( \mu S  H / N )$ which we show is optimal up to log-factors. In contrast, we show that any algorithm which does not interact with the MDP and uses an offline dataset of $N$ expert trajectories must incur suboptimality growing as $\Omega (S H^2/N)$ even under the $\mu$-recoverability assumption. This result establishes a clear and provable separation of the minimax suboptimality of IL when the learner can actively query the expert, and when the model is unknown on instances satisfying $\mu$-recoverability.

In this paper, we also study of IL going beyond the tabular setting, in the presence of /function approximation/. We initiate the study in the linear-expert setting, where the expert plays actions according to a linear classifier of known state-action features. This setting is a generalization of the linear-$Q^*$ setting when the expert policy is optimal. Here, we reduce IL to multi-class classification and show that with high probability, the suboptimality incurred by behavior cloning is in fact $\tilde{O} (dH^2/N)$. This is optimal up to log-factors, but can be improved to $\tilde{O} (dH/N)$ if we have a linear expert with /parameter-sharing/ across time steps.

In contrast, when the MDP transition structure is known to the learner such as in the case of simulators, we demonstrate a fundamental difference compared to the tabular setting in terms of the performance Mimic-MD when extended to the function approximation setting. We introduce a new problem called confidence set linear classification, that can be used to construct sample-efficient IL algorithms. Informally, in confidence set linear classification, the learner must output a classifier as well as a measurable set of inputs (confidence set) on which the output classifier /certifiably/ does not make a mistake. The loss incurred is the probability measure of the confidence set. The intuition behind this reduction is that a good confidence set linear classifier can be used to construct a large measure of states on which the expert action is known - this is amenable for the learner to generate large volumes of training data when the model is known by artificially rolling out the expert policy wherever possible. Indeed, we show that with a linear expert and with linearly realizable reward functions, a confidence set linear classification algorithm with worst case expected error $\le \epsilon$ can be used to instantiate Mimic-MD with suboptimality incurred $O (\epsilon H^{3/2} \sqrt{d/N})$.

We instantiate this reduction in the case of $|A| = 2$ actions and with state-action features are uniformly distributed on the unit sphere. Here, we show that the minimax rate of confidence set linear classification is $\tilde{\Theta} (d^{3/2} / N)$. As a corollary, this result establishes preliminary evidence towards breaking the quadratic-$H$ barrier going beyond the tabular setting.


4. *FastSecAgg: Scalable Secure Aggregation for Privacy-Preserving Federated Learning* ([https://arxiv.org/abs/2009.11248 ArXiv])

With /Swanand Kadhe/, /O. Ozan Koyluoglu/ and /Kannan Ramchandran/
