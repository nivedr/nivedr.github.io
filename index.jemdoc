# jemdoc: menu{MENU}{index.html}
# jemdoc: analytics{UA-130203500-1}

= Nived Rajaraman
~~~
{}{img_left}{images/nived-SD.jpg}{Temp}{300}

[https://eecs.berkeley.edu/ Electrical Engineering and Computer Science Department],\n
[https://www.berkeley.edu/ University of California, Berkeley]\n
264 Cory Hall\n
Berkeley, CA 94720-1770\n
Email: nived \[dot\] rajaraman \[at\] berkeley \[dot\] edu
~~~

== About me

I am currently a second year Ph.D. student, affiliated with the BLISS and BAIR labs. My advisors are Profs. Jiantao Jiao and Kannan Ramachandran.

My research interests lie in the theory of reinforcement learning. I believe that a succesful work in this area proposes concrete theoretical formulations and ultimately provides intuitions missing in existing practical approaches.

I was previously a dual degree student at the [http://www.ee.iitm.ac.in/ Department of Electrical Engineering], [https://www.iitm.ac.in/ IIT Madras]. I was fortunate to have [https://www.microsoft.com/en-us/research/people/rakri/ Ravishankar Krishnaswamy] and [http://www.ee.iitm.ac.in/~andrew/ Prof. Andrew Thangaraj] as my master's thesis advisors.


== Research directions

=== Imitation Learning


1. /Toward the Fundamental Limits of Imitation Learning/ (NeurIPS 2020)

With Lin F. Yang, Jiantao Jiao and Kannan Ramchandran

In sequential decision-making problems, Imitation learning (IL) posits to learn from the behavior of an expert given a collection of demonstrations, in the absence of reward feedback. In the past, several works have studied the performance of supervised learning approaches such as behavior cloning (Ross and Bagnell 2010), interactive approaches such as DAgger (Ross and Bagnell 2011) among others. However, from a theoretical point of view the statistical rates in even the simplest setting of episodic and tabular MDPs was not well understood.

In this paper, we show that when the learner is provided a dataset of N expert trajectories ahead of time, and cannot interact with the MDP, behavior cloning under log-loss is in fact $O( S H^2 log(N) / N)$ suboptimal compared to the value collected by the expert, even when the expert follows an arbitrary stochastic policy. Here, $S$ is the state space and $H$ is the length of the horizon. If the expert is constrained to be deterministic, this rate is in fact optimal up to logarithmic factors, /even if the learner is allowed to actively query the expert at visited states while interacting with the MDP/. What is more surprising, this rate does not bear any dependence on the number of actions $A$!

The quadratic dependence on the horizon $H$ is known as error-compounding in the literature and is often attributed to the problem of /distribution shift/ for behavior cloning. Indeed, the learner trains a good policy under the expert's (empirical) state distribution, but encounters states under their own rollout distribution. Our work shows that any algorithm, and not just supervised learning approaches, incurs error-compounding in the absence of MDP interaction. This begs the question, what is the /value of MDP interaction/ in Imitation Learning? Is it possible for a learner to break this $H^2$-dependence given access to an accurate model of the transition dynamics?

At first glance, knowledge of the MDP transition may not seem helpful as the learner does not know the actions played by the expert at states unvisited in the dataset. At best the learner can now distinguish between the distributions induced under different actions. However, we show that this view is rather myopic - in the presence of a good model, learners can abundantly generate artificial training data by rolling out the expert policy on known states. This motivates our novel algorithm, Mimic-MD which incurs a suboptimality $O(S H^{3/2}/N)$ when the model is known and the expert is deterministic.

This breaks the error-compounding barrier even in the worst case and shows a provable benefit to interacting with the MDP in terms of reducing the sample complexity requirement of the number of expert demonstrations. In the backdrop of this algorithm, we introduce a formal reduction of Imitation Learning with a known model to the problem of /uniform expert value estimation/ - estimating the value of the expert policy under all reward functions uniformly to bounded error. More details can be found on arXiv:

2. Provably Breaking the Quadratic Error Compounding Barrier in Imitation Learning, Optimally (under submission)

With Yanjun Han, Lin F. Yang, Kannan Ramchandran and Jiantao Jiao

With a known model, Mimic-MD provably breaks the error-compounding barrier in IL. However, it is a-priori unclear whether the dependence on the horizon, H^{3/2}, can be further improved given an accurate model. To this end, we establish a lower bound showing that for any learner which knows the MDP transition, there exists an IL instance on $|S| = 3$ states such that the suboptimality of the learner scales as $\Omega (H^{3/2}/N)$ with constant probability. This result shows that in terms of the horizon dependence, Mimic-MD is optimal in the worst case. The lower bound construction relies on a novel reduction to mean estimation with subsampled observations and establishing statistical limits for the same, which may be of independent interest.

Often in practice, the demonstrator turns out to carry out the task at hand very efficiently and is near optimal. In this work, we also explore IL under the additional restriction that the expert is an optimal policy under the true underlying reward function. This setting turns out to be quite challenging because of the highly non-convex reward structure imposed by the observations in the expert dataset. While it is trivial to construct good estimates of the expert state distribution, /realizing/ good state distributions via Markovian policies is a far more challenging problem. Achieving the latter is a necessary hurdle to overcome in coming up with policies with small suboptimality. In this paper, we propose an efficient algorithm, termed Mimic-Mixture, which can provably realize a near unbiased (upto an error of $O(1/N)$) state estimate of the state distribution of any single state in the MDP. As a consequence, for MDPs on 3-states with rewards only on the terminal layer, Mimic-Mixture returns a policy incurring suboptimality O(1/N).

In contrast, we show that no algorithm can achieve suboptimality $O(\sqrt{H/N})$ with high probability if the expert is not constrained to be optimal. Thus, our work formally establishes the benefit of imposing optimality of the expert when the model is known, which contrasts with the result in Rajaraman et al (2020) which shows that it does not help in the worst case, when the learner cannot interact with the environment.

3. On the Value of Interaction and FunctionApproximation in Imitation Learning (under submission)

With Yanjun Han, Lin F. Yang, Jiantao Jiao and Kannan Ramchandran

In practice, often learners implement algorithms such as DAgger which actively query the expert while interacting with the environment. In contrast, our previous work studies approaches such as behavior cloning as well as Mimic-MD which passively interact with the expert. In the worst case Rajaraman et al (2020) establishes that there is no benefit of actively querying the expert if the model is not known. The lower bound relies on constructing an MDP with a "bad" state which is absorbing and offers no reward and showing that any learner visits this state with reasonable probability. This worst case example is pathological in the following sense: in practical situations such as driving a car, experts often can recover at states and collect a high reward even if a "mistake" is made locally. This assumption is captured by the $\mu$-recoverability assumption of Ross and Bagnell (2011) which assumes that the difference in the $Q$-value under the expert policy across different actions in a state does not deviate by more than $\mu$ from the maximum.

In addition, Ross and Bagnell (2011) propose a reduction showing that any policy which minimizes the $0$-$1$ loss with the expert action distribution to $\le \epsilon$ when the state is generated by the learner's own state distribution incurs suboptimality $\le \mu H \epsilon$ under this assumption. In this work, we show that this reduction is in fact statistically optimal - the resulting policy upon interacting with the MDP for $N$ episodes incurs a suboptimality of $O ( \mu S  H / N )$ which we show is optimal up to log-factors. In contrast, we show that any algorithm which does not interact with the MDP and uses an offline dataset of $N$ expert trajectories must incur suboptimality growing as $\Omega (S H^2/N)$ even under the $\mu$-recoverability assumption. This result establishes a clear and provable separation of the minimax suboptimality of IL when the learner can actively query the expert, and when the model is unknown on instances satisfying $\mu$-recoverability.

In this paper, we also study of IL going beyond the tabular setting, in the presence of /function approximation/. We initiate the study in the linear-expert setting, where the expert plays actions according to a linear classifier of known state-action features. This setting is a generalization of the linear-$Q^*$ setting when the expert policy is optimal. Here, we reduce IL to multi-class classification and show that with high probability, the suboptimality incurred by behavior cloning is in fact $\tilde{O} (dH^2/N)$. This is optimal up to log-factors, but can be improved to $\tilde{O} (dH/N)$ if we have a linear expert with /parameter-sharing/ across time steps.

In contrast, when the MDP transition structure is known to the learner such as in the case of simulators, we demonstrate a fundamental difference compared to the tabular setting in terms of the performance Mimic-MD when extended to the function approximation setting. We introduce a new problem called confidence set linear classification, that can be used to construct sample-efficient IL algorithms. Informally, in confidence set linear classification, the learner must output a classifier as well as a measurable set of inputs (confidence set) on which the output classifier /certifiably/ does not make a mistake. The loss incurred is the probability measure of the confidence set. The intuition behind this reduction is that a good confidence set linear classifier can be used to construct a large measure of states on which the expert action is known - this is amenable for the learner to generate large volumes of training data when the model is known by artificially rolling out the expert policy wherever possible. Indeed, we show that with a linear expert and with linearly realizable reward functions, a confidence set linear classification algorithm with worst case expected error $\le \epsilon$ can be used to instantiate Mimic-MD with suboptimality incurred $O (\epsilon H^{3/2} \sqrt{d/N})$.

We instantiate this reduction in the case of $|A| = 2$ actions and with state-action features are uniformly distributed on the unit sphere. Here, we show that the minimax rate of confidence set linear classification is $\tilde{\Theta} (d^{3/2} / N)$. As a corollary, this result establishes preliminary evidence towards breaking the quadratic-$H$ barrier going beyond the tabular setting.


=== FastSecAgg: Scalable Secure Aggregation for Privacy-Preserving Federated Learning 

With Swanand Kadhe, O. Ozan Koyluoglu and Kannan Ramchandran
